{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nikhilnath\\Anaconda3\\envs\\myproject1\\lib\\site-packages\\ipykernel_launcher.py:24: UserWarning: \n",
      "This call to matplotlib.use() has no effect because the backend has already\n",
      "been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
      "or matplotlib.backends is imported for the first time.\n",
      "\n",
      "The backend was *originally* set to 'TkAgg' by the following code:\n",
      "  File \"C:\\Users\\Nikhilnath\\Anaconda3\\envs\\myproject1\\lib\\runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"C:\\Users\\Nikhilnath\\Anaconda3\\envs\\myproject1\\lib\\runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\Nikhilnath\\Anaconda3\\envs\\myproject1\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"C:\\Users\\Nikhilnath\\Anaconda3\\envs\\myproject1\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n",
      "    app.start()\n",
      "  File \"C:\\Users\\Nikhilnath\\Anaconda3\\envs\\myproject1\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 486, in start\n",
      "    self.io_loop.start()\n",
      "  File \"C:\\Users\\Nikhilnath\\Anaconda3\\envs\\myproject1\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 127, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Users\\Nikhilnath\\Anaconda3\\envs\\myproject1\\lib\\asyncio\\base_events.py\", line 422, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Users\\Nikhilnath\\Anaconda3\\envs\\myproject1\\lib\\asyncio\\base_events.py\", line 1432, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Users\\Nikhilnath\\Anaconda3\\envs\\myproject1\\lib\\asyncio\\events.py\", line 145, in _run\n",
      "    self._callback(*self._args)\n",
      "  File \"C:\\Users\\Nikhilnath\\Anaconda3\\envs\\myproject1\\lib\\site-packages\\tornado\\ioloop.py\", line 759, in _run_callback\n",
      "    ret = callback()\n",
      "  File \"C:\\Users\\Nikhilnath\\Anaconda3\\envs\\myproject1\\lib\\site-packages\\tornado\\stack_context.py\", line 276, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"C:\\Users\\Nikhilnath\\Anaconda3\\envs\\myproject1\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 536, in <lambda>\n",
      "    self.io_loop.add_callback(lambda : self._handle_events(self.socket, 0))\n",
      "  File \"C:\\Users\\Nikhilnath\\Anaconda3\\envs\\myproject1\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 450, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"C:\\Users\\Nikhilnath\\Anaconda3\\envs\\myproject1\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 480, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"C:\\Users\\Nikhilnath\\Anaconda3\\envs\\myproject1\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 432, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"C:\\Users\\Nikhilnath\\Anaconda3\\envs\\myproject1\\lib\\site-packages\\tornado\\stack_context.py\", line 276, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"C:\\Users\\Nikhilnath\\Anaconda3\\envs\\myproject1\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"C:\\Users\\Nikhilnath\\Anaconda3\\envs\\myproject1\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 233, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"C:\\Users\\Nikhilnath\\Anaconda3\\envs\\myproject1\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n",
      "    user_expressions, allow_stdin)\n",
      "  File \"C:\\Users\\Nikhilnath\\Anaconda3\\envs\\myproject1\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 208, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"C:\\Users\\Nikhilnath\\Anaconda3\\envs\\myproject1\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 537, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"C:\\Users\\Nikhilnath\\Anaconda3\\envs\\myproject1\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2662, in run_cell\n",
      "    raw_cell, store_history, silent, shell_futures)\n",
      "  File \"C:\\Users\\Nikhilnath\\Anaconda3\\envs\\myproject1\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2785, in _run_cell\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"C:\\Users\\Nikhilnath\\Anaconda3\\envs\\myproject1\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2903, in run_ast_nodes\n",
      "    if self.run_code(code, result):\n",
      "  File \"C:\\Users\\Nikhilnath\\Anaconda3\\envs\\myproject1\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2963, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-8-a64e2d404371>\", line 105, in <module>\n",
      "    main()\n",
      "  File \"<ipython-input-8-a64e2d404371>\", line 3, in main\n",
      "    plt.switch_backend('tkAgg')\n",
      "  File \"C:\\Users\\Nikhilnath\\Anaconda3\\envs\\myproject1\\lib\\site-packages\\matplotlib\\pyplot.py\", line 231, in switch_backend\n",
      "    matplotlib.use(newbackend, warn=False, force=True)\n",
      "  File \"C:\\Users\\Nikhilnath\\Anaconda3\\envs\\myproject1\\lib\\site-packages\\matplotlib\\__init__.py\", line 1410, in use\n",
      "    reload(sys.modules['matplotlib.backends'])\n",
      "  File \"C:\\Users\\Nikhilnath\\Anaconda3\\envs\\myproject1\\lib\\importlib\\__init__.py\", line 166, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"C:\\Users\\Nikhilnath\\Anaconda3\\envs\\myproject1\\lib\\site-packages\\matplotlib\\backends\\__init__.py\", line 16, in <module>\n",
      "    line for line in traceback.format_stack()\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib as mpl\n",
    "from matplotlib.image import imread\n",
    "from random import randint\n",
    "\n",
    "import theano\n",
    "import keras\n",
    "import pandas\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras import optimizers\n",
    "import keras.utils\n",
    "import keras.layers\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import copy\n",
    "import csv\n",
    "\n",
    "mpl.use('Agg')\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set y values of data to lie between 0 and 1\n",
    "def normalize_data(dataset, data_min, data_max):\n",
    "    data_std = (dataset - data_min) / (data_max - data_min)\n",
    "    test_scaled = data_std * (np.amax(data_std) - np.amin(data_std)) + np.amin(data_std)\n",
    "    return test_scaled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Import and pre-process data for future applications\n",
    "def import_data(train_dataframe, dev_dataframe, test_dataframe):\n",
    "    dataset = train_dataframe.values\n",
    "    dataset = dataset.astype('float32')\n",
    "\n",
    "    #Include all 12 initial factors (Year ; Month ; Hour ; Day ; Cloud Coverage ; Visibility ; Temperature ; Dew Point ;\n",
    "    #Relative Humidity ; Wind Speed ; Station Pressure ; Altimeter\n",
    "    max_test = np.max(dataset[:,12])\n",
    "    min_test = np.min(dataset[:,12])\n",
    "    scale_factor = max_test - min_test\n",
    "    max = np.empty(13)\n",
    "    min = np.empty(13)\n",
    "\n",
    "    #Create training dataset\n",
    "    for i in range(0,13):\n",
    "        min[i] = np.amin(dataset[:,i],axis = 0)\n",
    "        max[i] = np.amax(dataset[:,i],axis = 0)\n",
    "        dataset[:,i] = normalize_data(dataset[:, i], min[i], max[i])\n",
    "\n",
    "    train_data = dataset[:,0:12]\n",
    "    train_labels = dataset[:,12]\n",
    "\n",
    "    # Create dev dataset\n",
    "    dataset = dev_dataframe.values\n",
    "    dataset = dataset.astype('float32')\n",
    "\n",
    "    for i in range(0, 13):\n",
    "        dataset[:, i] = normalize_data(dataset[:, i], min[i], max[i])\n",
    "\n",
    "    dev_data = dataset[:,0:12]\n",
    "    dev_labels = dataset[:,12]\n",
    "\n",
    "    # Create test dataset\n",
    "    dataset = test_dataframe.values\n",
    "    dataset = dataset.astype('float32')\n",
    "\n",
    "    for i in range(0, 13):\n",
    "        dataset[:, i] = normalize_data(dataset[:, i], min[i], max[i])\n",
    "\n",
    "    test_data = dataset[:, 0:12]\n",
    "    test_labels = dataset[:, 12]\n",
    "\n",
    "    return train_data, train_labels, dev_data, dev_labels, test_data, test_labels, scale_factor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Construt and return Keras RNN model\n",
    "def build_model(init_type='glorot_uniform', optimizer='adam'):\n",
    "    model = Sequential()\n",
    "    layers = [12, 64, 64, 1, 1]\n",
    "    model.add(keras.layers.LSTM(\n",
    "        layers[0],\n",
    "        input_shape = (None,12),\n",
    "        return_sequences=True))\n",
    "    model.add(keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add(keras.layers.LSTM(\n",
    "        layers[1],\n",
    "        kernel_initializer = init_type,\n",
    "        return_sequences=True\n",
    "        #bias_initializer = 'zeros'\n",
    "    ))\n",
    "    model.add(keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(\n",
    "        layers[2], activation='tanh',\n",
    "        kernel_initializer=init_type,\n",
    "        input_shape = (None,1)\n",
    "        ))\n",
    "    model.add(Dense(\n",
    "        layers[3]))\n",
    "\n",
    "    model.add(Activation(\"relu\"))\n",
    "\n",
    "    #Alternative parameters:\n",
    "    momentum = 0.8\n",
    "    learning_rate = 0.1\n",
    "    epochs = 100\n",
    "    decay_rate = learning_rate / 100\n",
    "    sgd = keras.optimizers.SGD(lr=learning_rate, momentum=momentum, decay=decay_rate, nesterov=False)\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=sgd)\n",
    "    #rms = keras.optimizers.RMSprop(lr=0.002, rho=0.9, epsilon=1e-08, decay=0.01)\n",
    "    #model.compile(loss=\"mean_squared_error\", optimizer=optimizer)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Save output predictions for graphing and inspection\n",
    "def write_to_csv(prediction, filename):\n",
    "    print(\"Writing to CSV...\")\n",
    "    with open(filename, 'w') as file:\n",
    "        for i in range(prediction.shape[0]):\n",
    "            file.write(\"%.5f\" % prediction[i][0][0])\n",
    "            file.write('\\n')\n",
    "    print(\"...finished!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Return MSE error values of all three data sets based on a single model\n",
    "def evaluate(model, X_train, Y_train, X_dev, Y_dev, X_test, Y_test, scale_factor):\n",
    "    scores = model.evaluate(X_train, Y_train, verbose = 0) * scale_factor * scale_factor\n",
    "    print(\"train: \", model.metrics_names, \": \", scores)\n",
    "    scores = model.evaluate(X_dev, Y_dev, verbose = 0) * scale_factor * scale_factor\n",
    "    print(\"dev: \", model.metrics_names, \": \", scores)\n",
    "    scores = model.evaluate(X_test, Y_test, verbose = 0) * scale_factor * scale_factor\n",
    "    print(\"test: \", model.metrics_names, \": \", scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Calculate MSE between two arrays of values\n",
    "def mse(predicted, observed):\n",
    "    return np.sum(np.multiply((predicted - observed),(predicted - observed)))/predicted.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape:  (6028, 1, 12)  Y_train shape:  (6028, 1, 1)\n",
      "Epoch 1/100\n",
      "6028/6028 [==============================] - 1s 143us/step - loss: 9.8771\n",
      "Epoch 2/100\n",
      "6028/6028 [==============================] - 1s 121us/step - loss: 9.8918\n",
      "Epoch 3/100\n",
      "6028/6028 [==============================] - 1s 125us/step - loss: 9.8918\n",
      "Epoch 4/100\n",
      "6028/6028 [==============================] - 1s 123us/step - loss: 9.8918\n",
      "Epoch 5/100\n",
      "6028/6028 [==============================] - 1s 120us/step - loss: 9.8918\n",
      "Epoch 6/100\n",
      "6028/6028 [==============================] - 1s 120us/step - loss: 9.8918\n",
      "Epoch 7/100\n",
      "6028/6028 [==============================] - 1s 117us/step - loss: 9.8918\n",
      "Epoch 8/100\n",
      "6028/6028 [==============================] - 1s 118us/step - loss: 9.8918\n",
      "Epoch 9/100\n",
      "6028/6028 [==============================] - 1s 122us/step - loss: 9.8918\n",
      "Epoch 10/100\n",
      "6028/6028 [==============================] - 1s 120us/step - loss: 9.8918\n",
      "Epoch 11/100\n",
      "6028/6028 [==============================] - 1s 119us/step - loss: 9.8918\n",
      "Epoch 12/100\n",
      "6028/6028 [==============================] - 1s 120us/step - loss: 9.8918\n",
      "Epoch 13/100\n",
      "6028/6028 [==============================] - 1s 119us/step - loss: 9.8918\n",
      "Epoch 14/100\n",
      "6028/6028 [==============================] - 1s 122us/step - loss: 9.8918\n",
      "Epoch 15/100\n",
      "6028/6028 [==============================] - 1s 121us/step - loss: 9.8918\n",
      "Epoch 16/100\n",
      "6028/6028 [==============================] - 1s 125us/step - loss: 9.8918\n",
      "Epoch 17/100\n",
      "6028/6028 [==============================] - 1s 124us/step - loss: 9.8918\n",
      "Epoch 18/100\n",
      "6028/6028 [==============================] - 1s 124us/step - loss: 9.8918\n",
      "Epoch 19/100\n",
      "6028/6028 [==============================] - 1s 121us/step - loss: 9.8918\n",
      "Epoch 20/100\n",
      "6028/6028 [==============================] - 1s 131us/step - loss: 9.8918\n",
      "Epoch 21/100\n",
      "6028/6028 [==============================] - 1s 121us/step - loss: 9.8918\n",
      "Epoch 22/100\n",
      "6028/6028 [==============================] - 1s 123us/step - loss: 9.8918\n",
      "Epoch 23/100\n",
      "6028/6028 [==============================] - 1s 121us/step - loss: 9.8918\n",
      "Epoch 24/100\n",
      "6028/6028 [==============================] - 1s 125us/step - loss: 9.8918\n",
      "Epoch 25/100\n",
      "6028/6028 [==============================] - 1s 119us/step - loss: 9.8918\n",
      "Epoch 26/100\n",
      "6028/6028 [==============================] - 1s 119us/step - loss: 9.8918\n",
      "Epoch 27/100\n",
      "6028/6028 [==============================] - 1s 123us/step - loss: 9.8918\n",
      "Epoch 28/100\n",
      "6028/6028 [==============================] - 1s 123us/step - loss: 9.8918\n",
      "Epoch 29/100\n",
      "6028/6028 [==============================] - 1s 121us/step - loss: 9.8918\n",
      "Epoch 30/100\n",
      "6028/6028 [==============================] - 1s 122us/step - loss: 9.8918\n",
      "Epoch 31/100\n",
      "6028/6028 [==============================] - 1s 126us/step - loss: 9.8918\n",
      "Epoch 32/100\n",
      "6028/6028 [==============================] - 1s 121us/step - loss: 9.8918\n",
      "Epoch 33/100\n",
      "6028/6028 [==============================] - 1s 124us/step - loss: 9.8918\n",
      "Epoch 34/100\n",
      "6028/6028 [==============================] - 1s 119us/step - loss: 9.8918\n",
      "Epoch 35/100\n",
      "6028/6028 [==============================] - 1s 121us/step - loss: 9.8918\n",
      "Epoch 36/100\n",
      "6028/6028 [==============================] - 1s 123us/step - loss: 9.8918\n",
      "Epoch 37/100\n",
      "6028/6028 [==============================] - 1s 125us/step - loss: 9.8918\n",
      "Epoch 38/100\n",
      "6028/6028 [==============================] - 1s 122us/step - loss: 9.8918\n",
      "Epoch 39/100\n",
      "6028/6028 [==============================] - 1s 128us/step - loss: 9.8918\n",
      "Epoch 40/100\n",
      "6028/6028 [==============================] - 1s 140us/step - loss: 9.8918\n",
      "Epoch 41/100\n",
      "6028/6028 [==============================] - 1s 135us/step - loss: 9.8918\n",
      "Epoch 42/100\n",
      "6028/6028 [==============================] - 1s 121us/step - loss: 9.8918\n",
      "Epoch 43/100\n",
      "6028/6028 [==============================] - 1s 123us/step - loss: 9.8918\n",
      "Epoch 44/100\n",
      "6028/6028 [==============================] - 1s 124us/step - loss: 9.8918\n",
      "Epoch 45/100\n",
      "6028/6028 [==============================] - 1s 122us/step - loss: 9.8918\n",
      "Epoch 46/100\n",
      "6028/6028 [==============================] - 1s 119us/step - loss: 9.8918\n",
      "Epoch 47/100\n",
      "6028/6028 [==============================] - 1s 123us/step - loss: 9.8918\n",
      "Epoch 48/100\n",
      "6028/6028 [==============================] - 1s 121us/step - loss: 9.8918\n",
      "Epoch 49/100\n",
      "6028/6028 [==============================] - 1s 122us/step - loss: 9.8918\n",
      "Epoch 50/100\n",
      "6028/6028 [==============================] - 1s 119us/step - loss: 9.8918\n",
      "Epoch 51/100\n",
      "6028/6028 [==============================] - 1s 121us/step - loss: 9.8918\n",
      "Epoch 52/100\n",
      "6028/6028 [==============================] - 1s 122us/step - loss: 9.8918\n",
      "Epoch 53/100\n",
      "6028/6028 [==============================] - 1s 123us/step - loss: 9.8918\n",
      "Epoch 54/100\n",
      "6028/6028 [==============================] - 1s 136us/step - loss: 9.8918\n",
      "Epoch 55/100\n",
      "6028/6028 [==============================] - 1s 134us/step - loss: 9.8918\n",
      "Epoch 56/100\n",
      "6028/6028 [==============================] - 1s 123us/step - loss: 9.8918\n",
      "Epoch 57/100\n",
      "6028/6028 [==============================] - 1s 132us/step - loss: 9.8918\n",
      "Epoch 58/100\n",
      "6028/6028 [==============================] - 1s 124us/step - loss: 9.8918\n",
      "Epoch 59/100\n",
      "6028/6028 [==============================] - 1s 121us/step - loss: 9.8918\n",
      "Epoch 60/100\n",
      "6028/6028 [==============================] - 1s 121us/step - loss: 9.8918\n",
      "Epoch 61/100\n",
      "6028/6028 [==============================] - 1s 123us/step - loss: 9.8918\n",
      "Epoch 62/100\n",
      "6028/6028 [==============================] - 1s 127us/step - loss: 9.8918\n",
      "Epoch 63/100\n",
      "6028/6028 [==============================] - 1s 177us/step - loss: 9.8918\n",
      "Epoch 64/100\n",
      "6028/6028 [==============================] - 1s 122us/step - loss: 9.8918\n",
      "Epoch 65/100\n",
      "6028/6028 [==============================] - 1s 122us/step - loss: 9.8918 0s - los\n",
      "Epoch 66/100\n",
      "6028/6028 [==============================] - 1s 121us/step - loss: 9.8918\n",
      "Epoch 67/100\n",
      "6028/6028 [==============================] - 1s 126us/step - loss: 9.8918\n",
      "Epoch 68/100\n",
      "6028/6028 [==============================] - 1s 130us/step - loss: 9.8918\n",
      "Epoch 69/100\n",
      "6028/6028 [==============================] - 1s 148us/step - loss: 9.8918\n",
      "Epoch 70/100\n",
      "6028/6028 [==============================] - 1s 192us/step - loss: 9.8918 0s - loss:\n",
      "Epoch 71/100\n",
      "6028/6028 [==============================] - 1s 171us/step - loss: 9.8918\n",
      "Epoch 72/100\n",
      "6028/6028 [==============================] - 1s 175us/step - loss: 9.8918\n",
      "Epoch 73/100\n",
      "6028/6028 [==============================] - 1s 169us/step - loss: 9.8918\n",
      "Epoch 74/100\n",
      "6028/6028 [==============================] - 1s 144us/step - loss: 9.8918\n",
      "Epoch 75/100\n",
      "6028/6028 [==============================] - 1s 128us/step - loss: 9.8918\n",
      "Epoch 76/100\n",
      "6028/6028 [==============================] - 1s 132us/step - loss: 9.8918\n",
      "Epoch 77/100\n",
      "6028/6028 [==============================] - 1s 131us/step - loss: 9.8918\n",
      "Epoch 78/100\n",
      "6028/6028 [==============================] - 1s 136us/step - loss: 9.8918\n",
      "Epoch 79/100\n",
      "6028/6028 [==============================] - 1s 131us/step - loss: 9.8918\n",
      "Epoch 80/100\n",
      "6028/6028 [==============================] - 1s 141us/step - loss: 9.8918\n",
      "Epoch 81/100\n",
      "6028/6028 [==============================] - 1s 138us/step - loss: 9.8918\n",
      "Epoch 82/100\n",
      "6028/6028 [==============================] - 1s 130us/step - loss: 9.8918\n",
      "Epoch 83/100\n",
      "6028/6028 [==============================] - 1s 136us/step - loss: 9.8918\n",
      "Epoch 84/100\n",
      "6028/6028 [==============================] - 1s 138us/step - loss: 9.8918\n",
      "Epoch 85/100\n",
      "6028/6028 [==============================] - 1s 121us/step - loss: 9.8918\n",
      "Epoch 86/100\n",
      "6028/6028 [==============================] - 1s 119us/step - loss: 9.8918\n",
      "Epoch 87/100\n",
      "6028/6028 [==============================] - 1s 135us/step - loss: 9.8918\n",
      "Epoch 88/100\n",
      "6028/6028 [==============================] - 1s 128us/step - loss: 9.8918\n",
      "Epoch 89/100\n",
      "6028/6028 [==============================] - 1s 119us/step - loss: 9.8918\n",
      "Epoch 90/100\n",
      "6028/6028 [==============================] - 1s 136us/step - loss: 9.8918\n",
      "Epoch 91/100\n",
      "6028/6028 [==============================] - 1s 122us/step - loss: 9.8918\n",
      "Epoch 92/100\n",
      "6028/6028 [==============================] - 1s 118us/step - loss: 9.8918\n",
      "Epoch 93/100\n",
      "6028/6028 [==============================] - 1s 148us/step - loss: 9.8918\n",
      "Epoch 94/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6028/6028 [==============================] - 1s 133us/step - loss: 9.8918\n",
      "Epoch 95/100\n",
      "6028/6028 [==============================] - 1s 128us/step - loss: 9.8918\n",
      "Epoch 96/100\n",
      "6028/6028 [==============================] - 1s 174us/step - loss: 9.8918\n",
      "Epoch 97/100\n",
      "6028/6028 [==============================] - 1s 142us/step - loss: 9.8918\n",
      "Epoch 98/100\n",
      "6028/6028 [==============================] - 1s 128us/step - loss: 9.8918\n",
      "Epoch 99/100\n",
      "6028/6028 [==============================] - 1s 122us/step - loss: 9.8918\n",
      "Epoch 100/100\n",
      "6028/6028 [==============================] - 1s 121us/step - loss: 9.8918\n",
      "Train MSE:  1048666364.4002267\n",
      "Dev MSE:  1046596384.5662687\n",
      "Test MSE:  1052988525.0399587\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'fit'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-a64e2d404371>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 105\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-17-a64e2d404371>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;31m#cv_model = KerasClassifier(build_fn=build_model, epochs=100, batch_size=16, verbose=0)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[0madaboost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAdaBoostClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malgorithm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'SAMME'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m     \u001b[0madaboost\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[0mtrainset_predicted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madaboost\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\myproject1\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    411\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    412\u001b[0m         \u001b[1;31m# Fit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 413\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAdaBoostClassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    414\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    415\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_validate_estimator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\myproject1\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m         \u001b[1;31m# Check parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_estimator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m         \u001b[1;31m# Clear any previous fit results\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\myproject1\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py\u001b[0m in \u001b[0;36m_validate_estimator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    427\u001b[0m                     \u001b[1;34m\"Please change the base estimator or set \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m                     \"algorithm='SAMME' instead.\")\n\u001b[1;32m--> 429\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhas_fit_parameter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase_estimator_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"sample_weight\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    430\u001b[0m             raise ValueError(\"%s doesn't support sample_weight.\"\n\u001b[0;32m    431\u001b[0m                              % self.base_estimator_.__class__.__name__)\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\myproject1\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mhas_fit_parameter\u001b[1;34m(estimator, parameter)\u001b[0m\n\u001b[0;32m    660\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    661\u001b[0m     \"\"\"\n\u001b[1;32m--> 662\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mparameter\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msignature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    663\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    664\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'function' object has no attribute 'fit'"
     ]
    }
   ],
   "source": [
    "\n",
    "def main():\n",
    "    plt.switch_backend('tkAgg')\n",
    "\n",
    "    #Import test data (6027, 13)\n",
    "    train_dataframe = pandas.read_csv('weather_train.txt', sep=\";\", engine='python', header = None)\n",
    "    #train_dataframe = np.random.random((1000, 100))\n",
    "    dev_dataframe = pandas.read_csv('weather_dev.txt', sep=\";\", engine='python', header = None)\n",
    "    test_dataframe = pandas.read_csv('weather_test.txt', sep=\";\", engine='python', header = None)\n",
    "    train_data, train_labels, dev_data, dev_labels, test_data, test_labels, scale_factor = import_data(train_dataframe, dev_dataframe, test_dataframe)\n",
    "\n",
    "    X_train = np.reshape(train_data, (train_data.shape[0], 1, train_data.shape[1]))\n",
    "    X_dev = np.reshape(dev_data, (dev_data.shape[0], 1, dev_data.shape[1]))\n",
    "    X_test = np.reshape(test_data, (test_data.shape[0], 1, test_data.shape[1]))\n",
    "    Y_train = np.reshape(train_labels, (train_labels.shape[0], 1, 1))\n",
    "    Y_dev = np.reshape(dev_labels, (dev_labels.shape[0], 1, 1))\n",
    "    Y_test = np.reshape(test_labels, (test_labels.shape[0], 1, 1))\n",
    "\n",
    "    model = build_model('glorot_uniform', 'adam')\n",
    "\n",
    "    #Standard vanilla LSTM model\n",
    "\n",
    "    model_fit_epochs = 100\n",
    "    print(\"X_train shape: \",X_train.shape, \" Y_train shape: \",Y_train.shape)\n",
    "\n",
    "    model.fit(\n",
    "        X_train, Y_train,\n",
    "        batch_size = 16, epochs = model_fit_epochs)\n",
    "    trainset_predicted = model.predict(X_train)\n",
    "    devset_predicted = model.predict(X_dev)\n",
    "    testset_predicted = model.predict(X_test)\n",
    "\n",
    "    print(\"Train MSE: \", mse(trainset_predicted, Y_train) * scale_factor * scale_factor)\n",
    "    print(\"Dev MSE: \", mse(devset_predicted, Y_dev) * scale_factor * scale_factor)\n",
    "    print(\"Test MSE: \", mse(testset_predicted, Y_test) * scale_factor * scale_factor)\n",
    "\n",
    "    #Adaboost model (ensemble learning)\n",
    "    \n",
    "    #cv_model = KerasClassifier(build_fn=build_model, epochs=100, batch_size=16, verbose=0)\n",
    "    adaboost = AdaBoostClassifier(base_estimator=build_model, learning_rate=0.01, algorithm='SAMME')\n",
    "    adaboost.fit(train_data, train_labels)\n",
    "\n",
    "    trainset_predicted = adaboost.predict(X_train)\n",
    "    devset_predicted = adaboost.predict(X_dev)\n",
    "    testset_predicted = adaboost.predict(X_test)\n",
    "\n",
    "    print(\"Train MSE: \", mse(trainset_predicted, Y_train) * scale_factor * scale_factor)\n",
    "    print(\"Dev MSE: \", mse(devset_predicted, Y_dev) * scale_factor * scale_factor)\n",
    "    print(\"Test MSE: \", mse(testset_predicted, Y_test) * scale_factor * scale_factor)\n",
    "\n",
    "    # K-fold cross validation (K = 10):\n",
    "    kf = KFold(n_splits=10, shuffle=True)\n",
    "    # Loop through the indices the split() method returns\n",
    "    for index, (train_indices, val_indices) in enumerate(kf.split(X_train, Y_train)):\n",
    "        print(\"Training on fold \" + str(index + 1) + \"/10...\")\n",
    "        # Generate batches from indices\n",
    "        xtrain, xval = X_train[train_indices], X_train[val_indices]\n",
    "        ytrain, yval = Y_train[train_indices], Y_train[val_indices]\n",
    "        # Clear model, and create it\n",
    "        model = None\n",
    "        model = build_model()\n",
    "\n",
    "        model.fit(\n",
    "            xtrain, ytrain,\n",
    "            batch_size=16, epochs=model_fit_epochs)\n",
    "        testset_predicted = model.predict(xval)\n",
    "        print(\"Test MSE: \", mse(testset_predicted, yval))\n",
    "\n",
    "    #Grid search to optimize model params\n",
    "\n",
    "    init = ['glorot_uniform', 'normal', 'uniform']\n",
    "    epochs = [50, 100, 150]\n",
    "    batches = [8, 16, 32]\n",
    "#    optimizers = ['rmsprop', 'adam', 'adaboost']\n",
    "    optimizers = ['rmsprop', 'adam', 'sgd']\n",
    "    optimal_params = np.empty(4)\n",
    "    minimum_error = 2e63\n",
    "\n",
    "    for init_type in init:\n",
    "        for epoch in epochs:\n",
    "            for batch in batches:\n",
    "                for optimizer in optimizers:\n",
    "                    model = None\n",
    "                    model = build_model(init_type, optimizer)\n",
    "\n",
    "                    model.fit(\n",
    "                        X_train, Y_train,\n",
    "                        batch_size=batch, epochs=epoch)\n",
    "                    predicted = model.predict(X_test)\n",
    "                    error = mse(predicted, Y_test)\n",
    "                    if error < minimum_error:\n",
    "                        error = minimum_error\n",
    "                        optimal_params = [init_type, epoch, batch, optimizer]\n",
    "\n",
    "    print(\"optimal params: \", optimal_params)\n",
    "    print(\"minimized error: \", minimum_error)\n",
    "\n",
    "    write_to_csv(trainset_predicted,'nn_trainset_prediction.csv')\n",
    "    write_to_csv(devset_predicted,'nn_devset_prediction.csv')\n",
    "    write_to_csv(testset_predicted, 'nn_testset_prediction.csv')\n",
    "\n",
    "    return\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(2e63)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
